<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Chen Shangyu</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2018-02-04T13:43:40+08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Mark Otto</name>
   <email></email>
 </author>

 
 <entry>
   <title>Network Sparsity</title>
   <link href="http://localhost:4000/2017/11/05/Network-Sparsity/"/>
   <updated>2017-11-05T00:00:00+08:00</updated>
   <id>http://localhost:4000/2017/11/05/Network-Sparsity</id>
   <content type="html">&lt;p&gt;This article is about some methods to sparsify neural network by training.&lt;/p&gt;

&lt;p&gt;Recently I have read two paper published by &lt;a href=&quot;http://www.sungjuhwang.com/&quot;&gt;Sung Ju Hwang&lt;/a&gt; group in ICML 2017. I think these two papers share some ideas. Put them here to be compared.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1 id=&quot;combined-group-and-exclusive-sparisty-for-deep-neural-network&quot;&gt;Combined Group and Exclusive Sparisty for Deep Neural Network&lt;/h1&gt;

&lt;h2 id=&quot;paper-in-one-sentence&quot;&gt;Paper in one sentence&lt;/h2&gt;
&lt;p&gt;This paper adds two regularizers: &lt;strong&gt;Group sparsity&lt;/strong&gt; and &lt;strong&gt;exclusive sparsity&lt;/strong&gt;, to train neural network.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;In the optimal case, the weights at each layer will be fully orthogonal to each other, and thus forming an orthogonal basis set.&lt;/li&gt;
  &lt;li&gt;To do so, enforce network weights at each layer to fit to different sets of input features as much as possible. (exclusive sparsity)&lt;/li&gt;
  &lt;li&gt;However, it is not practical nor desirable to restrict each weight to be completely disjoint from others as some features still need to be shared (think about low level features). Therefore introduce an additional group sparsity regularizer based on (2, 1)-norm. (group sparsity)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;idea-visiualization&quot;&gt;Idea Visiualization&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/Network-Sparsity/main_idea.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Group Sparsity tries to delete input neurons.&lt;/li&gt;
  &lt;li&gt;Exclusive Sparsity tries to make output neurons fight for input neurons. That is to say input neurons belong to only one output neuron.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;formulation&quot;&gt;Formulation&lt;/h2&gt;
&lt;h3 id=&quot;group-sparsity&quot;&gt;Group Sparsity&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\Omega (\mathbf{W}) = \sum_g ||\mathbf{W}_g^l||_2 = \sum_g \sqrt{\sum_i (w_{g,i}^l)^2}&lt;/script&gt;
&lt;br /&gt;
where:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; represents group, in this paper, every input neuron and its corresponding weights are a group.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;: layer of neural network&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Obviously we can see, it does a &lt;strong&gt;group lasso&lt;/strong&gt; in input neuron: do 2-norm at group first, then do 1-norm between groups.&lt;/p&gt;

&lt;h3 id=&quot;exclusive-sparsity&quot;&gt;Exclusive Sparsity&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\Omega (\mathbf{W}) = \frac{1}{2} \sum_g ||\mathbf{W}_g^l||_1^2 = \frac{1}{2} \sum_g (\sum_i |w_{g,i}^l|)^2&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;some-thinkings&quot;&gt;Some thinkings&lt;/h3&gt;
&lt;p&gt;It is easy to knwo using group lasso can result in group sparsity. But it confuses me why using (1,2)-norm can “make output neurons compete for inputs”.&lt;/p&gt;

&lt;p&gt;As is said in paper:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Applying 2-norm over these 1-norm groups will result in even weights among the groups; that is, all groups should have similar number of non-sparse weights, and thus no group can have large number of non-sparse weight.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Also, exclusive sparsity was first introduced in &lt;strong&gt;Multi-task learning&lt;/strong&gt;: &lt;a href=&quot;https://dennyzhou.github.io/papers/HSVM.pdf&quot;&gt;Hierarchical Classification via Orthogonal Transfer&lt;/a&gt; Maybe can find more details in this paper.&lt;/p&gt;

&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;
&lt;p&gt;It uses &lt;strong&gt;Proximal Gradient Descent&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First obtains the intermediate solution &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}_{t+\frac{1}{2}}&lt;/script&gt; by taking a gradient step using the gradient computed on the loss only.&lt;/li&gt;
  &lt;li&gt;Then optimize for the regularization term while performing Euclidean projection of it to the solution space:
&lt;script type=&quot;math/tex&quot;&gt;\min_{\mathbf{W}_{t+1}} = \{\Omega (\mathbf{W}_{t+\frac{1}{2}})  + \frac{1}{2\lambda} || \mathbf{W}_{t+1} - \mathbf{W}_{t+\frac{1}{2} }||_2^2 \}&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;group-sparsity-proximal-step&quot;&gt;Group Sparsity proximal step&lt;/h3&gt;
&lt;p&gt;Set 
&lt;script type=&quot;math/tex&quot;&gt;f(\mathbf{W}_{t+1}) = \frac{1}{2} ||\mathbf{W}_{t+1} - \mathbf{W}_{t + \frac{1}{2}}||^2_2 + \underbrace{\lambda [\mathbf{W}_{t+ 1}^1, \mathbf{W}_{t+ 1}^2, ... ,\mathbf{W}_{t+ 1}^g]}_{\text{Regularizer}}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial f(\mathbf{W}_{t+1})}{\partial \mathbf{W}_{t+1}} = \mathbf{W}_{t+1} - \mathbf{W}_{t + \frac{1}{2}} + \lambda [\frac{\mathbf{W}_{t+1}^1}{||\mathbf{W}_{t+1}^1||_2}, \frac{\mathbf{W}_{t+ 1}^2}{||\mathbf{W}_{t+ 1}^2||_2}, ... ,\frac{\mathbf{W}_{t+ 1}^g}{||\mathbf{W}_{t+ 1}^g||}] = 0&lt;/script&gt;

&lt;p&gt;Finally we get:
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}^{t+1}_{g,i} = \left(1 - \frac{\lambda}{||\mathbf{W}_g||_2} \right)_{+} \mathbf{W}^{t + \frac{1}{2}}_{g,i}&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;exclusive-sparsity-1&quot;&gt;Exclusive Sparsity&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{W}^{t+1}_{g,i} = \left(1 - \frac{\lambda ||\mathbf{W}_g||_1}{|w_{g,i}|} \right)_{+} \mathbf{W}^{t+\frac{1}{2}}_{g,i}&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&quot;visualizationfully-connected&quot;&gt;Visualization–Fully Connected&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/images/Network-Sparsity/visualization1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Group sparsity regularizer results in the total elimination of certain features.&lt;/li&gt;
  &lt;li&gt;Exclusive sparsity regularizer, when used on its own, results in disjoint feature selection for each class.&lt;/li&gt;
  &lt;li&gt;When combined, it allows certain degree of feature reuse&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;visualizationconvolution&quot;&gt;Visualization–Convolution&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/images/Network-Sparsity/visualization_conv.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Combined group and exclusive sparsity regularizer results in filters that are much sharper than others.&lt;/li&gt;
  &lt;li&gt;Some spatial features dropped altogether from the competition with other filters.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>BirdEye - an Automatic Method for Inverse Perspective Transformation of Road Image without Calibration</title>
   <link href="http://localhost:4000/2015/07/09/IPM/"/>
   <updated>2015-07-09T00:00:00+08:00</updated>
   <id>http://localhost:4000/2015/07/09/IPM</id>
   <content type="html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Inverse Perspective Mapping(IPM) based lane detection is widely employed in vehicle intelligence applications.
Currently, most IPM method requires the camera to be calibrated in advance. Any shifts of camera will lead to the failure of the previous calibration. In this work, a calibration-free approach is proposed to iteratively attain an accurate inverse perspective transformation of the road, through which a “birdeye” view of the target road, as well as parallel lanes, can be gained. This method obtains 4 end-points of a pair of lanes in perspective image by our lane detection algorithm first. Based on the hypothesis that the road is flat, we project these points to the corresponding points in the IPM view in which the two lanes are parallel lines to get the initial transformation matrix. Then, we use an iteration strategy to increase IPM’s accuracy by minimizing the detection 
error between the lines in two views. After the iteration stops, the better transformation matrix is used for final IPM. Besides, to attain a better IPM view in which every lane approaches parallel, we propose a multi-lanes based IPM which involve more lanes. The results of several experiments are provided to demonstrate the effectiveness of the method.&lt;/p&gt;

&lt;h2 id=&quot;algorithm-flow-chart&quot;&gt;Algorithm Flow Chart&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/IPM/flowchart.jpg&quot; alt=&quot;Algorithm flow chart&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;algorithm-overview&quot;&gt;Algorithm Overview&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/IPM/AlgorithmOverview.png&quot; alt=&quot;Algorithm Overview&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;inverse-perspective-mapping&quot;&gt;Inverse Perspective Mapping&lt;/h2&gt;
&lt;p&gt;In computer vision, the mathematical relationship between two planes is defined as a homography matrix H. As explained 
in [1], the matrix H can be expressed as H = sMR. We can attain the transformation relationship between two planes by 8 
corresponding points, 4 points in each plane. The image below illustrates the process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/IPM/8points.png&quot; alt=&quot;8 points mapping&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pre-process&quot;&gt;Pre Process&lt;/h2&gt;

&lt;h3 id=&quot;denoising-based-on-lanes-characteristic&quot;&gt;Denoising based on Lane’s characteristic&lt;/h3&gt;
&lt;p&gt;The lanes in perspective view possess some different feature from other lines, such as the lanes’ angles are always
within a certain range. Therefore, we need to set a threshold to filtrate all the lines, wiping off nearly-horizontal and
nearly-perpendicular lines. Moreover, the noise lines are minimal and separated in different frame of the road record.
It is also useful to filter out the lines which are minority based on their angles from the total lines gathered in a 
number of frames. The rest lines are the lanes we want.&lt;/p&gt;

&lt;h3 id=&quot;bisect-kmeans-to-distinguish-different-lanes&quot;&gt;Bisect-Kmeans to Distinguish Different Lanes&lt;/h3&gt;
&lt;p&gt;Real road condition is complex, we can hardly know how many lanes in the road, which will cause great trouble to our process. 
Therefore, it is necessary to cluster the lines in time. In this paper we put forward a fast method especially aimed at lane 
detection to get the number of the clusters. In real road image, the horizontal ordinate of points which lines cross the upper 
screen possess big difference from each other. Therefore, we can perform a fast bisect-Kmeans based on crossing points: divide 
all the lines in two part with Kmeans, and calculate each cluster’s variance also based on the the horizontal ordinate of the 
crossing points. If its variance is bigger than a certain threshold, keep Kmeans(K=2) it.&lt;/p&gt;

&lt;h2 id=&quot;iterative-method-fo-ipm&quot;&gt;Iterative Method fo IPM&lt;/h2&gt;
&lt;p&gt;After the first-time IPM, we detect the IPM view to locate the lanes’ end points (we name them “project points”), which should 
be the real corresponding points of perspective view’s end points. Then transform the “project points” back into the perspective 
view as “back points”. However, there maybe some inaccuracy in our IPM view’s detection, thus we can not use the back points to 
replace the source points. Instead, we combine the back points with the previous source points to get new source points (The 
combining method can be various: predict-combine or weights-combine), then use these new points to start iteration. Therefore by 
iteration, more accurate IPM view can be attained.&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/IPM/ExperimentResult.jpg&quot; alt=&quot;Experiment Result&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] Bradski,G,”Learning OpenCV”,O’REILLY,2008&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A Saliency SIFT Feature-Based Method for Image Recommendation</title>
   <link href="http://localhost:4000/2015/07/08/Saliency-SIFT-image-recommendation/"/>
   <updated>2015-07-08T00:00:00+08:00</updated>
   <id>http://localhost:4000/2015/07/08/Saliency-SIFT-image-recommendation</id>
   <content type="html">&lt;h2 id=&quot;abtract&quot;&gt;Abtract&lt;/h2&gt;
&lt;p&gt;Current image search and image recommendation show their boundedness in accuracy, because these methods tend to neglect images’ content while focus on textual information searching in the Internet. In order to fully employ images’ information, such as color, style, texture to perform recommendation which is more similar to human’s recognition, a saliency-based SIFT feature extracted method is proposed to acquire detailed information of images. What’s more, a bag-of-words model is applied to better represent images. Finally based on the feature extracted, our recommendation method takes advantage of SVM to depict images’ possibility to certain image category to calculateimage’s distance to users, which approximates human’s view in comparing images. What’s more, a method to extend imagecategories to achieve accurate classification is put forward.&lt;/p&gt;

&lt;h2 id=&quot;algorithm-overview&quot;&gt;Algorithm Overview&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/ImageRecommendation/Saliency-SIFT algorithm.png&quot; alt=&quot;AlgorithmOverview&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;saliency&quot;&gt;Saliency&lt;/h2&gt;
&lt;p&gt;In our paper, we used the method in [1] to achieve saliency detection with a robust result. Firstly, a new background measure from a conceptual perspective is derived and then an effective computation method is described. Secondly, the method further discusses the unique benefits originating from its intuitive geometrical interpretation. Finally, the method proposes a principled framework that intuitively integrates low level cues and directly aims for the goal to combine multiple saliency cues or measures.&lt;/p&gt;

&lt;h2 id=&quot;sift-bag-of-words&quot;&gt;SIFT bag-of-words&lt;/h2&gt;
&lt;h3 id=&quot;sift-feature&quot;&gt;SIFT feature&lt;/h3&gt;
&lt;p&gt;Scale-invariant feature transform (SIFT) algorithm transforms an image into a large collection of feature vectors, each of which is invariant to image translation, scaling, and rotation,partially invariant to illumination changes and robust to localgeometric distortion.&lt;/p&gt;

&lt;h3 id=&quot;bag-of-words-model&quot;&gt;Bag-of-words Model&lt;/h3&gt;
&lt;p&gt;The bag-of-words(BOW) model originates from natural language processing and information retrieval, commonly used in methods of document classification, where the (frequency of) occurrence of each word is used as a feature for training a classifier.&lt;/p&gt;

&lt;p&gt;In our method, SIFT features will be firstly detected. Every image is described as a M * 128 matrix (M represents the number of the SIFT detected). However, different numbers of SIFT features are extracted in one image, which cause difficulty in processing. To solve the problem of dimensional unequality, all image’s SIFT features are collected and clustered into K clusters (K is the user definite) to form the codebook which is a K*128 matrix, and every row is the centroid of a cluster). Codebook can be viewed as a standard of features where every image’s SIFT features find its nearest centroid in the codebook.&lt;/p&gt;

&lt;p&gt;After all the features of a image are mapped to the codebook, a K dimension histogram is used to record the frequency of every matched codeword, representing the image in the BOW model. Algorithm.1 describes the process.&lt;/p&gt;

&lt;h3 id=&quot;svm-based-similarity&quot;&gt;SVM Based Similarity&lt;/h3&gt;
&lt;p&gt;Images contains various objects and partitions. Even art master can not definitely state two images are similar, while people tend to attribute images to certain categories or how much it belongs to certain categories. Therefore, assumption can be made that images are similar because that they belongs to the same category or they partly belongs to some same categories. Therefore, we propose a similarity calculation method to measure the distance between two images by multiplying how much image i belongs to category c and how much image j belongs to category c for all the categories.&lt;/p&gt;

&lt;h2 id=&quot;recommendation-system&quot;&gt;Recommendation System&lt;/h2&gt;
&lt;p&gt;To define our recommendation algorithm, first to define the records used in our methods. As a user browses images (In our experiment, what the users browse come from training set), the system will record the image index that the user get interested in. For a given user, record will be {imagei; imagej; imagek; …}. For a given record, the algorithm first specifies the user’s reference, or how much the user favors certain category.&lt;/p&gt;

&lt;p&gt;For instance, the algorithm will calculate the categories distribution in the records, category one for 40%, category two for 30% etc. For every image in the recommended set (In our experiment,
the test set), the algorithm will calculate their distance from the user by summing up image’s possibility to certain category multiply how much the user are in favor of this category.&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/images/ImageRecommendation/ExperimentResult.jpg&quot; alt=&quot;ExperimentResult&quot; /&gt;
The figure above demonstrate 5 set of experiment results. Every set contain 5 input images and 3 output(recommended images). The left column images are users’ interested images while the right ones are recommended. The display order of recommended images is according to the how strongly the system want to recommend the image to the user. The left ones are the most recommended images.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] W. Zhu, S. Liang, Y. Wei, and J. Sun, “Saliency optimization from robust background detection,” in Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, 2014, pp. 2814 – 2821.&lt;/p&gt;
</content>
 </entry>
 

</feed>
