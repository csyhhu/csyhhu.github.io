<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
                    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                            });
  </script>
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      A Saliency SIFT Feature-Based Method for Image Recommendation &middot; Chen Shangyu
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon1.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-08">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Chen Shangyu
        </a>
      </h1>
      <p class="lead">PhD student at Nanyang Technological University, Singapore</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/articles/">Article list</a>
          
        
      
        
      
        
          
        
      
      <!--
      <a class="sidebar-nav-item" href="https://github.com/csyhhu/archive/v2.1.0.zip">Download</a>
      -->
      <a class="sidebar-nav-item" href= "https://github.com/csyhhu" >GitHub</a>
      <!--
      <span class="sidebar-nav-item">Currently v2.1.0</span>
      -->
    </nav>
    <!--
    <p>&copy; 2018. All rights reserved.</p>
    -->
  </div>
</div>

    
    <div class="content container">
      <div class="post">
  <h1 class="post-title">A Saliency SIFT Feature-Based Method for Image Recommendation</h1>
  <span class="post-date">08 Jul 2015</span>
  <h2 id="abtract">Abtract</h2>
<p>Current image search and image recommendation show their boundedness in accuracy, because these methods tend to neglect images’ content while focus on textual information searching in the Internet. In order to fully employ images’ information, such as color, style, texture to perform recommendation which is more similar to human’s recognition, a saliency-based SIFT feature extracted method is proposed to acquire detailed information of images. What’s more, a bag-of-words model is applied to better represent images. Finally based on the feature extracted, our recommendation method takes advantage of SVM to depict images’ possibility to certain image category to calculateimage’s distance to users, which approximates human’s view in comparing images. What’s more, a method to extend imagecategories to achieve accurate classification is put forward.</p>

<h2 id="algorithm-overview">Algorithm Overview</h2>
<p><img src="/images/ImageRecommendation/Saliency-SIFT algorithm.png" alt="AlgorithmOverview" /></p>

<h2 id="saliency">Saliency</h2>
<p>In our paper, we used the method in [1] to achieve saliency detection with a robust result. Firstly, a new background measure from a conceptual perspective is derived and then an effective computation method is described. Secondly, the method further discusses the unique benefits originating from its intuitive geometrical interpretation. Finally, the method proposes a principled framework that intuitively integrates low level cues and directly aims for the goal to combine multiple saliency cues or measures.</p>

<h2 id="sift-bag-of-words">SIFT bag-of-words</h2>
<h3 id="sift-feature">SIFT feature</h3>
<p>Scale-invariant feature transform (SIFT) algorithm transforms an image into a large collection of feature vectors, each of which is invariant to image translation, scaling, and rotation,partially invariant to illumination changes and robust to localgeometric distortion.</p>

<h3 id="bag-of-words-model">Bag-of-words Model</h3>
<p>The bag-of-words(BOW) model originates from natural language processing and information retrieval, commonly used in methods of document classification, where the (frequency of) occurrence of each word is used as a feature for training a classifier.</p>

<p>In our method, SIFT features will be firstly detected. Every image is described as a M * 128 matrix (M represents the number of the SIFT detected). However, different numbers of SIFT features are extracted in one image, which cause difficulty in processing. To solve the problem of dimensional unequality, all image’s SIFT features are collected and clustered into K clusters (K is the user definite) to form the codebook which is a K*128 matrix, and every row is the centroid of a cluster). Codebook can be viewed as a standard of features where every image’s SIFT features find its nearest centroid in the codebook.</p>

<p>After all the features of a image are mapped to the codebook, a K dimension histogram is used to record the frequency of every matched codeword, representing the image in the BOW model. Algorithm.1 describes the process.</p>

<h3 id="svm-based-similarity">SVM Based Similarity</h3>
<p>Images contains various objects and partitions. Even art master can not definitely state two images are similar, while people tend to attribute images to certain categories or how much it belongs to certain categories. Therefore, assumption can be made that images are similar because that they belongs to the same category or they partly belongs to some same categories. Therefore, we propose a similarity calculation method to measure the distance between two images by multiplying how much image i belongs to category c and how much image j belongs to category c for all the categories.</p>

<h2 id="recommendation-system">Recommendation System</h2>
<p>To define our recommendation algorithm, first to define the records used in our methods. As a user browses images (In our experiment, what the users browse come from training set), the system will record the image index that the user get interested in. For a given user, record will be {imagei; imagej; imagek; …}. For a given record, the algorithm first specifies the user’s reference, or how much the user favors certain category.</p>

<p>For instance, the algorithm will calculate the categories distribution in the records, category one for 40%, category two for 30% etc. For every image in the recommended set (In our experiment,
the test set), the algorithm will calculate their distance from the user by summing up image’s possibility to certain category multiply how much the user are in favor of this category.</p>

<h2 id="experiment">Experiment</h2>
<p><img src="/images/ImageRecommendation/ExperimentResult.jpg" alt="ExperimentResult" />
The figure above demonstrate 5 set of experiment results. Every set contain 5 input images and 3 output(recommended images). The left column images are users’ interested images while the right ones are recommended. The display order of recommended images is according to the how strongly the system want to recommend the image to the user. The left ones are the most recommended images.</p>

<h2 id="reference">Reference</h2>
<p>[1] W. Zhu, S. Liang, Y. Wei, and J. Sun, “Saliency optimization from robust background detection,” in Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, 2014, pp. 2814 – 2821.</p>


</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2017/11/05/Network-Sparsity/">
            Network Sparsity
            <small>05 Nov 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/07/09/IPM/">
            BirdEye - an Automatic Method for Inverse Perspective Transformation of Road Image without Calibration
            <small>09 Jul 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>


  </body>
</html>
