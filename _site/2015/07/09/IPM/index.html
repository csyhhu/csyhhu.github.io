<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
                    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                            });
  </script>
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      BirdEye - an Automatic Method for Inverse Perspective Transformation of Road Image without Calibration &middot; Chen Shangyu
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon1.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-08">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Chen Shangyu
        </a>
      </h1>
      <p class="lead">PhD student at Nanyang Technological University, Singapore</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/articles/">Article list</a>
          
        
      
        
      
        
          
        
      
      <!--
      <a class="sidebar-nav-item" href="https://github.com/csyhhu/archive/v2.1.0.zip">Download</a>
      -->
      <a class="sidebar-nav-item" href= "https://github.com/csyhhu" >GitHub</a>
      <!--
      <span class="sidebar-nav-item">Currently v2.1.0</span>
      -->
    </nav>
    <!--
    <p>&copy; 2018. All rights reserved.</p>
    -->
  </div>
</div>

    
    <div class="content container">
      <div class="post">
  <h1 class="post-title">BirdEye - an Automatic Method for Inverse Perspective Transformation of Road Image without Calibration</h1>
  <span class="post-date">09 Jul 2015</span>
  <h2 id="abstract">Abstract</h2>
<p>Inverse Perspective Mapping(IPM) based lane detection is widely employed in vehicle intelligence applications.
Currently, most IPM method requires the camera to be calibrated in advance. Any shifts of camera will lead to the failure of the previous calibration. In this work, a calibration-free approach is proposed to iteratively attain an accurate inverse perspective transformation of the road, through which a “birdeye” view of the target road, as well as parallel lanes, can be gained. This method obtains 4 end-points of a pair of lanes in perspective image by our lane detection algorithm first. Based on the hypothesis that the road is flat, we project these points to the corresponding points in the IPM view in which the two lanes are parallel lines to get the initial transformation matrix. Then, we use an iteration strategy to increase IPM’s accuracy by minimizing the detection 
error between the lines in two views. After the iteration stops, the better transformation matrix is used for final IPM. Besides, to attain a better IPM view in which every lane approaches parallel, we propose a multi-lanes based IPM which involve more lanes. The results of several experiments are provided to demonstrate the effectiveness of the method.</p>

<h2 id="algorithm-flow-chart">Algorithm Flow Chart</h2>
<p><img src="/images/IPM/flowchart.jpg" alt="Algorithm flow chart" /></p>

<h2 id="algorithm-overview">Algorithm Overview</h2>
<p><img src="/images/IPM/AlgorithmOverview.png" alt="Algorithm Overview" /></p>

<h2 id="inverse-perspective-mapping">Inverse Perspective Mapping</h2>
<p>In computer vision, the mathematical relationship between two planes is defined as a homography matrix H. As explained 
in [1], the matrix H can be expressed as H = sMR. We can attain the transformation relationship between two planes by 8 
corresponding points, 4 points in each plane. The image below illustrates the process.</p>

<p><img src="/images/IPM/8points.png" alt="8 points mapping" /></p>

<h2 id="pre-process">Pre Process</h2>

<h3 id="denoising-based-on-lanes-characteristic">Denoising based on Lane’s characteristic</h3>
<p>The lanes in perspective view possess some different feature from other lines, such as the lanes’ angles are always
within a certain range. Therefore, we need to set a threshold to filtrate all the lines, wiping off nearly-horizontal and
nearly-perpendicular lines. Moreover, the noise lines are minimal and separated in different frame of the road record.
It is also useful to filter out the lines which are minority based on their angles from the total lines gathered in a 
number of frames. The rest lines are the lanes we want.</p>

<h3 id="bisect-kmeans-to-distinguish-different-lanes">Bisect-Kmeans to Distinguish Different Lanes</h3>
<p>Real road condition is complex, we can hardly know how many lanes in the road, which will cause great trouble to our process. 
Therefore, it is necessary to cluster the lines in time. In this paper we put forward a fast method especially aimed at lane 
detection to get the number of the clusters. In real road image, the horizontal ordinate of points which lines cross the upper 
screen possess big difference from each other. Therefore, we can perform a fast bisect-Kmeans based on crossing points: divide 
all the lines in two part with Kmeans, and calculate each cluster’s variance also based on the the horizontal ordinate of the 
crossing points. If its variance is bigger than a certain threshold, keep Kmeans(K=2) it.</p>

<h2 id="iterative-method-fo-ipm">Iterative Method fo IPM</h2>
<p>After the first-time IPM, we detect the IPM view to locate the lanes’ end points (we name them “project points”), which should 
be the real corresponding points of perspective view’s end points. Then transform the “project points” back into the perspective 
view as “back points”. However, there maybe some inaccuracy in our IPM view’s detection, thus we can not use the back points to 
replace the source points. Instead, we combine the back points with the previous source points to get new source points (The 
combining method can be various: predict-combine or weights-combine), then use these new points to start iteration. Therefore by 
iteration, more accurate IPM view can be attained.</p>

<h2 id="experiment">Experiment</h2>
<p><img src="/images/IPM/ExperimentResult.jpg" alt="Experiment Result" /></p>

<h2 id="reference">Reference</h2>
<p>[1] Bradski,G,”Learning OpenCV”,O’REILLY,2008</p>


</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2017/11/05/Network-Sparsity/">
            Network Sparsity
            <small>05 Nov 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/07/08/Saliency-SIFT-image-recommendation/">
            A Saliency SIFT Feature-Based Method for Image Recommendation
            <small>08 Jul 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>


  </body>
</html>
