<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
                    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
                            });
  </script>
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Network Sparsity &middot; Chen Shangyu
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon1.png">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-08">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Chen Shangyu
        </a>
      </h1>
      <p class="lead">PhD student at Nanyang Technological University, Singapore</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/articles/">Article list</a>
          
        
      
        
      
        
          
        
      
      <!--
      <a class="sidebar-nav-item" href="https://github.com/csyhhu/archive/v2.1.0.zip">Download</a>
      -->
      <a class="sidebar-nav-item" href= "https://github.com/csyhhu" >GitHub</a>
      <!--
      <span class="sidebar-nav-item">Currently v2.1.0</span>
      -->
    </nav>
    <!--
    <p>&copy; 2018. All rights reserved.</p>
    -->
  </div>
</div>

    
    <div class="content container">
      <div class="post">
  <h1 class="post-title">Network Sparsity</h1>
  <span class="post-date">05 Nov 2017</span>
  <p>This article is about some methods to sparsify neural network by training.</p>

<p>Recently I have read two paper published by <a href="http://www.sungjuhwang.com/">Sung Ju Hwang</a> group in ICML 2017. I think these two papers share some ideas. Put them here to be compared.</p>

<!--more-->

<h1 id="combined-group-and-exclusive-sparisty-for-deep-neural-network">Combined Group and Exclusive Sparisty for Deep Neural Network</h1>

<h2 id="paper-in-one-sentence">Paper in one sentence</h2>
<p>This paper adds two regularizers: <strong>Group sparsity</strong> and <strong>exclusive sparsity</strong>, to train neural network.</p>

<h2 id="motivation">Motivation</h2>
<ul>
  <li>In the optimal case, the weights at each layer will be fully orthogonal to each other, and thus forming an orthogonal basis set.</li>
  <li>To do so, enforce network weights at each layer to fit to different sets of input features as much as possible. (exclusive sparsity)</li>
  <li>However, it is not practical nor desirable to restrict each weight to be completely disjoint from others as some features still need to be shared (think about low level features). Therefore introduce an additional group sparsity regularizer based on (2, 1)-norm. (group sparsity)</li>
</ul>

<h2 id="idea-visiualization">Idea Visiualization</h2>
<p><img src="/images/Network-Sparsity/main_idea.png" alt="" /></p>

<ul>
  <li>Group Sparsity tries to delete input neurons.</li>
  <li>Exclusive Sparsity tries to make output neurons fight for input neurons. That is to say input neurons belong to only one output neuron.</li>
</ul>

<h2 id="formulation">Formulation</h2>
<h3 id="group-sparsity">Group Sparsity</h3>
<p><script type="math/tex">\Omega (\mathbf{W}) = \sum_g ||\mathbf{W}_g^l||_2 = \sum_g \sqrt{\sum_i (w_{g,i}^l)^2}</script>
<br />
where:</p>

<ul>
  <li><script type="math/tex">g</script> represents group, in this paper, every input neuron and its corresponding weights are a group.</li>
  <li><script type="math/tex">l</script>: layer of neural network</li>
</ul>

<p>Obviously we can see, it does a <strong>group lasso</strong> in input neuron: do 2-norm at group first, then do 1-norm between groups.</p>

<h3 id="exclusive-sparsity">Exclusive Sparsity</h3>
<p><script type="math/tex">\Omega (\mathbf{W}) = \frac{1}{2} \sum_g ||\mathbf{W}_g^l||_1^2 = \frac{1}{2} \sum_g (\sum_i |w_{g,i}^l|)^2</script></p>

<h3 id="some-thinkings">Some thinkings</h3>
<p>It is easy to knwo using group lasso can result in group sparsity. But it confuses me why using (1,2)-norm can “make output neurons compete for inputs”.</p>

<p>As is said in paper:</p>
<blockquote>
  <p>Applying 2-norm over these 1-norm groups will result in even weights among the groups; that is, all groups should have similar number of non-sparse weights, and thus no group can have large number of non-sparse weight.</p>
</blockquote>

<p>Also, exclusive sparsity was first introduced in <strong>Multi-task learning</strong>: <a href="https://dennyzhou.github.io/papers/HSVM.pdf">Hierarchical Classification via Orthogonal Transfer</a> Maybe can find more details in this paper.</p>

<h2 id="optimization">Optimization</h2>
<p>It uses <strong>Proximal Gradient Descent</strong>:</p>

<ol>
  <li>First obtains the intermediate solution <script type="math/tex">\mathbf{W}_{t+\frac{1}{2}}</script> by taking a gradient step using the gradient computed on the loss only.</li>
  <li>Then optimize for the regularization term while performing Euclidean projection of it to the solution space:
<script type="math/tex">\min_{\mathbf{W}_{t+1}} = \{\Omega (\mathbf{W}_{t+\frac{1}{2}})  + \frac{1}{2\lambda} || \mathbf{W}_{t+1} - \mathbf{W}_{t+\frac{1}{2} }||_2^2 \}</script></li>
</ol>

<h3 id="group-sparsity-proximal-step">Group Sparsity proximal step</h3>
<p>Set 
<script type="math/tex">f(\mathbf{W}_{t+1}) = \frac{1}{2} ||\mathbf{W}_{t+1} - \mathbf{W}_{t + \frac{1}{2}}||^2_2 + \underbrace{\lambda [\mathbf{W}_{t+ 1}^1, \mathbf{W}_{t+ 1}^2, ... ,\mathbf{W}_{t+ 1}^g]}_{\text{Regularizer}}</script></p>

<script type="math/tex; mode=display">\frac{\partial f(\mathbf{W}_{t+1})}{\partial \mathbf{W}_{t+1}} = \mathbf{W}_{t+1} - \mathbf{W}_{t + \frac{1}{2}} + \lambda [\frac{\mathbf{W}_{t+1}^1}{||\mathbf{W}_{t+1}^1||_2}, \frac{\mathbf{W}_{t+ 1}^2}{||\mathbf{W}_{t+ 1}^2||_2}, ... ,\frac{\mathbf{W}_{t+ 1}^g}{||\mathbf{W}_{t+ 1}^g||}] = 0</script>

<p>Finally we get:
<script type="math/tex">\mathbf{W}^{t+1}_{g,i} = \left(1 - \frac{\lambda}{||\mathbf{W}_g||_2} \right)_{+} \mathbf{W}^{t + \frac{1}{2}}_{g,i}</script></p>

<h3 id="exclusive-sparsity-1">Exclusive Sparsity</h3>
<p><script type="math/tex">\mathbf{W}^{t+1}_{g,i} = \left(1 - \frac{\lambda ||\mathbf{W}_g||_1}{|w_{g,i}|} \right)_{+} \mathbf{W}^{t+\frac{1}{2}}_{g,i}</script></p>

<h2 id="experiments">Experiments</h2>
<h3 id="visualizationfully-connected">Visualization–Fully Connected</h3>
<p><img src="/images/Network-Sparsity/visualization1.png" alt="" /></p>

<ul>
  <li>Group sparsity regularizer results in the total elimination of certain features.</li>
  <li>Exclusive sparsity regularizer, when used on its own, results in disjoint feature selection for each class.</li>
  <li>When combined, it allows certain degree of feature reuse</li>
</ul>

<h3 id="visualizationconvolution">Visualization–Convolution</h3>
<p><img src="/images/Network-Sparsity/visualization_conv.png" alt="" /></p>

<ul>
  <li>Combined group and exclusive sparsity regularizer results in filters that are much sharper than others.</li>
  <li>Some spatial features dropped altogether from the competition with other filters.</li>
</ul>



</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2015/07/09/IPM/">
            BirdEye - an Automatic Method for Inverse Perspective Transformation of Road Image without Calibration
            <small>09 Jul 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/07/08/Saliency-SIFT-image-recommendation/">
            A Saliency SIFT Feature-Based Method for Image Recommendation
            <small>08 Jul 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>


  </body>
</html>
